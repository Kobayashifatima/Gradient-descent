{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12e1992",
   "metadata": {},
   "source": [
    "# gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a simple cost function (quadratic function)\n",
    "def cost_function(theta):\n",
    "    return theta**2 + 5\n",
    "\n",
    "# Define the gradient of the cost function\n",
    "def gradient(theta):\n",
    "    return 2 * theta\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(learning_rate, num_iterations):\n",
    "    theta = np.random.rand()  # Initialize theta with a random value\n",
    "    theta_history = [theta]  # Store the history of theta values\n",
    "    cost_history = [cost_function(theta)]  # Store the history of cost values\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient_value = gradient(theta)  # Compute the gradient\n",
    "        theta = theta - learning_rate * gradient_value  # Update theta\n",
    "        theta_history.append(theta)\n",
    "        cost_history.append(cost_function(theta))\n",
    "\n",
    "    return theta_history, cost_history\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_iterations = 50\n",
    "\n",
    "# Perform Gradient Descent\n",
    "theta_history, cost_history = gradient_descent(learning_rate, num_iterations)\n",
    "\n",
    "# Plot the cost function and the path taken by gradient descent\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(theta_history, cost_history, marker='o')\n",
    "plt.xlabel('Theta')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(theta_history, marker='o')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Theta')\n",
    "plt.title('Gradient Descent Path')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66607150",
   "metadata": {},
   "source": [
    "anatomy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6a4f6",
   "metadata": {},
   "source": [
    "Anatomy of Gradient descent\n",
    "Gradient descent is an optimization algorithm used in machine learning and deep learning to minimize the cost or loss function of a model by iteratively adjusting the model's parameters. It's a fundamental concept in the training of neural networks and other optimization problems. Here's the anatomy of gradient descent:\n",
    "\n",
    "1. **Objective Function (Cost/Loss Function)**:\n",
    "   - Gradient descent begins with an objective function, often denoted as J(θ), where θ represents the parameters of the model.\n",
    "   - The objective is to minimize this function by finding the optimal values of θ.\n",
    "\n",
    "2. **Initialization**:\n",
    "   - Gradient descent starts by initializing the model parameters θ with some values.\n",
    "   - Common initialization methods include setting θ to zeros, random values, or predefined values.\n",
    "\n",
    "3. **Learning Rate (α)**:\n",
    "   - The learning rate (α) is a hyperparameter that determines the step size at each iteration.\n",
    "   - It's a critical parameter as it influences the convergence and stability of the algorithm. Choosing an appropriate learning rate is crucial.\n",
    "\n",
    "4. **Gradient Calculation**:\n",
    "   - The key step in gradient descent is to compute the gradient of the cost function with respect to the parameters (∇J(θ)).\n",
    "   - The gradient points in the direction of the steepest increase in the cost function.\n",
    "\n",
    "5. **Update Parameters**:\n",
    "   - The parameters θ are updated using the gradient and learning rate:\n",
    "     θ = θ - α * ∇J(θ)\n",
    "   - The subtraction ensures that the algorithm moves in the direction of decreasing cost.\n",
    "\n",
    "6. **Iteration**:\n",
    "   - Steps 4 and 5 are repeated iteratively until a stopping criterion is met. This can be a fixed number of iterations, a target cost value, or other convergence criteria.\n",
    "\n",
    "7. **Convergence**:\n",
    "   - Gradient descent converges when it reaches a point where the gradient is close to zero or when it meets the predefined stopping criterion.\n",
    "   - Convergence implies that the parameters θ have reached values that minimize the cost function.\n",
    "\n",
    "8. **Cost Monitoring**:\n",
    "   - Typically, the cost function is monitored during training. The goal is to observe the cost decreasing over iterations, indicating that the algorithm is converging.\n",
    "\n",
    "9. **Types of Gradient Descent**:\n",
    "   - There are different variants of gradient descent, including:\n",
    "     - **Batch Gradient Descent**: It computes the gradient using the entire training dataset in each iteration.\n",
    "     - **Stochastic Gradient Descent (SGD)**: It computes the gradient using one training example at a time, making it faster but with more variance.\n",
    "     - **Mini-Batch Gradient Descent**: A compromise between batch and stochastic gradient descent, where a small random subset of data is used in each iteration.\n",
    "\n",
    "10. **Regularization and Optimization Techniques**:\n",
    "    - In practice, gradient descent is often used with additional techniques like L1 and L2 regularization, momentum, and adaptive learning rates (e.g., Adam, RMSprop) to improve convergence and generalization.\n",
    "\n",
    "11. **Termination**:\n",
    "    - Gradient descent terminates when the stopping criterion is met, and the trained model parameters are used for predictions.\n",
    "\n",
    "12. **Challenges**:\n",
    "    - Gradient descent may get stuck in local minima, and the choice of the learning rate is crucial.\n",
    "    - Poorly conditioned or ill-posed problems can also affect gradient descent.\n",
    "\n",
    "Overall, gradient descent is a fundamental algorithm in machine learning and deep learning, and its successful application requires careful tuning of hyperparameters and an understanding of the data and problem at hand.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
